
@INPROCEEDINGS{Moore99,
  AUTHOR =       "R. Moore and J. Lopes",
  TITLE =        "Paper templates",
  BOOKTITLE =    "TEMPLATE'06, 1st International Conference on Template Production",
  YEAR =         "1999",
  publisher =    "SCITEPRESS",
  file = F
}

@BOOK{Smith98,
  AUTHOR =       "J. Smith",
  TITLE =        "The Book",
  PUBLISHER =    "The publishing company",
  YEAR =         "1998",
  address =      "London",
  edition =      "2nd",
  file = F
}

@article{PFRL,
	author  = {Yasuhiro Fujita and Prabhat Nagarajan and Toshiki Kataoka and Takahiro Ishikawa},
	title   = {ChainerRL: A Deep Reinforcement Learning Library},
	journal = {Journal of Machine Learning Research},
	year    = {2021},
	volume  = {22},
	number  = {77},
	pages   = {1-14},
	url     = {http://jmlr.org/papers/v22/20-376.html}
}

@misc{EuropeanCommission2021,
	author =       {European Commission},
	title =        {Smart cities},
	URL={https://ec.europa.eu/info/eu-regional-and-urban-development/topics/cities-and-urban-development/city-initiatives/smart-cities_en},
	year =         {2021},
}

@misc{SUMBA,
	author =       "Project SUMBA",
	title =        "GUIDANCE FOR TRANSPORT MODELLING AND DATA COLLECTION",
	URL={https://www.eltis.org/resources/tools/guidance-transport-modelling-and-transport-data-collection-intermodality},
	year =         "2020",
}

@book{ModellingTransport,
	author = {Ortúzar, Juan de Dios and Willumsen, Luis},
	year = {2011},
	month = {06},
	pages = {i-xix},
	title = {Modelling Transport, Fourth Edition},
	isbn = {978-0-470-76039-0},
	doi = {10.1002/9781119993308.fmatter}
}

@book{HandbookTransportModelling,
	author = {Button, Kenneth and Hensher, David},
	year = {2007},
	month = {01},
	pages = {},
	publisher={Emerald Publishing},
	title = {Handbook of Transport Modelling},
	isbn = {978-008-045376-7},
	volume = {1}
}

@Inbook{Innes2011,
	author="Innes, John
	and Kouhy, Reza",
	editor="Abdel-Kader, Magdy G.",
	title="The Activity-Based Approach",
	bookTitle="Review of Management Accounting Research",
	year="2011",
	publisher="Palgrave Macmillan UK",
	address="London",
	pages="243--274",
	abstract="The activity-based (AB) approach concentrates on overhead costs, which are a significant percentage of total costs for many organizations in both the private and public sectors. In the 1980s the AB approach concentrated on product or service costing with a two-stage process. First, overheads are pooled on the basis of activities (such as material handling and purchasing) which consume resources. Second, products or services consume activities and cost drivers (such as the number of material movements and number of purchase orders) are the links between activity cost pools and product (or service) lines. One initial attraction of AB costing was the different unit product (or service) costs calculated on an AB basis from the unit product (or service) costs calculated using traditional overhead costing. Details of the mechanics of AB costing can be found in Innes and Mitchell (1992).",
	isbn="978-0-230-35327-5",
	doi="10.1057/9780230353275_10",
	url="https://doi.org/10.1057/9780230353275_10"
}

@article{SUMMO,
	author = {Krajzewicz, Daniel and Erdmann, Jakob and Behrisch, Michael and Bieker-Walz, Laura},
	year = {2012},
	month = {12},
	pages = {},
	title = {Recent Development and Applications of SUMO - Simulation of Urban MObility},
	volume = {3 and 4},
	journal = {International Journal On Advances in Systems and Measurements}
}

@article{Calt,
	author = {California Department of Transportation},
	year = {2021},
	title = {Caltrans Performance Measurement System},
	URL={https://pems.dot.ca.gov/}
}

@article{ASAM,
	author = {Association for Standardization of Automation and Measuring Systems},
	year = {2021},
	title = {ASAM OpenDRIVE},
	URL={https://https://www.asam.net/standards/detail/opendrive/}
}

@book{MATSim,
	author = {Horni A., Nagel, K., and Axhausen K.W},
	year = {2016},
	month = {01},
	pages = {},
	publisher={Ubiquity Press},
	title = {The Multi-Agent Transport Simulation},
	doi = {DOI: http://dx.doi.org/10.5334/baw},
	volume = {1}
}

@misc{dosovitskiy2017carla,
	title={CARLA: An Open Urban Driving Simulator}, 
	author={Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
	year={2017},
	eprint={1711.03938},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{BOLT,
author={BOLT},
year=2020,
title={Simulating cities for a better ride-hailing experience at Bolt},
URL={https://medium.com/bolt-labs/simulating-cities-for-a-better-ride-hailing-experience-at-bolt-f97af9190ada}
}


@misc{OSRM,
	author={OSRM},
	year=2020,
	title={Routing Machine Project OSRM},
	URL={http://project-osrm.org}
}

@inproceedings{
	ault2021reinforcement,
	title={Reinforcement Learning Benchmarks for Traffic Signal Control},
	author={James Ault and Guni Sharon},
	booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
	year={2021},
	url={https://openreview.net/forum?id=LqRSh6V0vR}
}

@inproceedings{WiVre04,
	author = {Wiering, Marco and Vreeken, Jilles and Veenen, J. and Koopman, Arne},
	year = {2004},
	month = {07},
	pages = {453 - 458},
	title = {Simulation and optimization of traffic in a city},
	isbn = {0-7803-8310-9},
	journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
	doi = {10.1109/IVS.2004.1336426}
}

@article{WALRAVEN2016203,
	title = {Traffic flow optimization: A reinforcement learning approach},
	journal = {Engineering Applications of Artificial Intelligence},
	volume = {52},
	pages = {203-212},
	year = {2016},
	issn = {0952-1976},
	doi = {https://doi.org/10.1016/j.engappai.2016.01.001},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197616000038},
	author = {Erwin Walraven and Matthijs T.J. Spaan and Bram Bakker},
	keywords = {Traffic flow optimization, Traffic congestion, Variable speed limits, Reinforcement learning, Neural networks},
	abstract = {Traffic congestion causes important problems such as delays, increased fuel consumption and additional pollution. In this paper we propose a new method to optimize traffic flow, based on reinforcement learning. We show that a traffic flow optimization problem can be formulated as a Markov Decision Process. We use Q-learning to learn policies dictating the maximum driving speed that is allowed on a highway, such that traffic congestion is reduced. An important difference between our work and existing approaches is that we take traffic predictions into account. A series of simulation experiments shows that the resulting policies significantly reduce traffic congestion under high traffic demand, and that inclusion of traffic predictions improves the quality of the resulting policies. Additionally, the policies are sufficiently robust to deal with inaccurate speed and density measurements.}
}

@misc{GenRaz16,
	title={Using a Deep Reinforcement Learning Agent for Traffic Signal Control}, 
	author={Wade Genders and Saiedeh Razavi},
	year={2016},
	eprint={1611.01142},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@ARTICLE{Cas17,
	title={Deep Reinforcement Learning for Urban Traffic Light Control}, 
	author={N. Casas},
	year={2017},
    journal={Master Thesis in Advanced Artificial Intelligence, Department of Artificial Intelligence Universidad Nacional de Educación a Distancia}
}

@misc{LiDa,
	title={An Efficient Deep Reinforcement Learning Model for Urban Traffic Control}, 
	author={Yilun Lin and Xingyuan Dai and Li Li and Fei-Yue Wang},
	year={2018},
	eprint={1808.01876},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}

@ARTICLE{Wu20,
	author={Wu, Tong and Zhou, Pan and Liu, Kai and Yuan, Yali and Wang, Xiumin and Huang, Huawei and Wu, Dapeng Oliver},
	journal={IEEE Transactions on Vehicular Technology}, 
	title={Multi-Agent Deep Reinforcement Learning for Urban Traffic Light Control in Vehicular Networks}, 
	year={2020},
	volume={69},
	number={8},
	pages={8243-8256},
	doi={10.1109/TVT.2020.2997896}}

@article{HeZRS15,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Deep Residual Learning for Image Recognition},
	journal   = {CoRR},
	volume    = {abs/1512.03385},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.03385},
	eprinttype = {arXiv},
	eprint    = {1512.03385},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{schulman2017proximal,
	title={Proximal Policy Optimization Algorithms}, 
	author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	year={2017},
	eprint={1707.06347},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{lillicrap2019continuous,
	title={Continuous control with deep reinforcement learning}, 
	author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	year={2019},
	eprint={1509.02971},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{suttonbook,
	author = {S. Sutton, Richard and G. Barto, Andrew},
	title = {Reinforcement Learning: An Introduction Second Edition},
	year = 2018,
	publisher={The MIT Press}
}

@misc{MnBA,
	title={Asynchronous Methods for Deep Reinforcement Learning}, 
	author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	year={2016},
	eprint={1602.01783},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{MPLight, title={Toward A Thousand Lights: Decentralized Deep Reinforcement Learning for Large-Scale Traffic Signal Control}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5744}, DOI={10.1609/aaai.v34i04.5744}, abstractNote={&lt;p&gt;Traffic congestion plagues cities around the world. Recent years have witnessed an unprecedented trend in applying reinforcement learning for traffic signal control. However, the primary challenge is to control and coordinate traffic lights in large-scale urban networks. No one has ever tested RL models on a network of more than a thousand traffic lights. In this paper, we tackle the problem of multi-intersection traffic signal control, especially for large-scale networks, based on RL techniques and transportation theories. This problem is quite difficult because there are challenges such as scalability, signal coordination, data feasibility, etc. To address these challenges, we (1) design our RL agents utilizing ‘pressure’ concept to achieve signal coordination in region-level; (2) show that implicit coordination could be achieved by individual control agents with well-crafted reward design thus reducing the dimensionality; and (3) conduct extensive experiments on multiple scenarios, including a real-world scenario with 2510 traffic lights in Manhattan, New York City &lt;sup&gt;1&lt;/sup&gt; &lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;}, number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Chacha and Wei, Hua and Xu, Nan and Zheng, Guanjie and Yang, Ming and Xiong, Yuanhao and Xu, Kai and Li, Zhenhui}, year={2020}, month={Apr.}, pages={3414-3421} }

@misc{ault,
	title={Learning an Interpretable Traffic Signal Control Policy}, 
	author={James Ault and Josiah P. Hanna and Guni Sharon},
	year={2020},
	eprint={1912.11023},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{zheng2019learning,
	title={Learning Phase Competition for Traffic Signal Control}, 
	author={Guanjie Zheng and Yuanhao Xiong and Xinshi Zang and Jie Feng and Hua Wei and Huichu Zhang and Yong Li and Kai Xu and Zhenhui Li},
	year={2019},
	eprint={1905.04722},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{liang2018rllib,
	title={RLlib: Abstractions for Distributed Reinforcement Learning}, 
	author={Eric Liang and Richard Liaw and Philipp Moritz and Robert Nishihara and Roy Fox and Ken Goldberg and Joseph E. Gonzalez and Michael I. Jordan and Ion Stoica},
	year={2018},
	eprint={1712.09381},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}

@misc{TFAgents,
	title = {{TF-Agents}: A library for Reinforcement Learning in TensorFlow},
	author = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and
	Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and
	Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and
	Jamie Smith and Gábor Bartók and Jesse Berent and Chris Harris and
	Vincent Vanhoucke and Eugene Brevdo},
	howpublished = {\url{https://github.com/tensorflow/agents}},
	url = "https://github.com/tensorflow/agents",
	year = 2018,
	note = "[Online; accessed 25-June-2019]"
}

@misc{baselines,
	author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
	title = {OpenAI Baselines},
	year = {2017},
	publisher = {GitHub},
	journal = {GitHub repository},
	howpublished = {\url{https://github.com/openai/baselines}},
}

@article{terry2020pettingzoo,
	Title = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
	Author = {Terry, J. K and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sulivan, Ryan and Santos, Luis and Perez, Rodrigo and Horsch, Caroline and Dieffendahl, Clemens and Williams, Niall L and Lokesh, Yashas and Sullivan, Ryan and Ravi, Praveen},
	journal={arXiv preprint arXiv:2009.14471},
	year={2020}
}

@misc{gym,
	Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	Title = {OpenAI Gym},
	Year = {2016},
	Eprint = {arXiv:1606.01540},
}

@article{SuperSuit,
	Title = {SuperSuit: Simple Microwrappers for Reinforcement Learning Environments},
	Author = {Terry, Justin K and Black, Benjamin and Hari, Ananth},
	journal={arXiv preprint arXiv:2008.08932},
	year={2020}
}

@misc{terry2021agent,
	title={Agent Environment Cycle Games}, 
	author={Justin K Terry and Nathaniel Grammel and Benjamin Black and Ananth Hari and Caroline Horsch and Luis Santos},
	year={2021},
	eprint={2009.13051},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{DBLP:conf/ijcai/TomasekHABC21,
	author    = {Petr Tom{\'{a}}sek and
	Karel Hor{\'{a}}k and
	Aditya Aradhye and
	Branislav Bosansk{\'{y}} and
	Krishnendu Chatterjee},
	editor    = {Zhi{-}Hua Zhou},
	title     = {Solving Partially Observable Stochastic Shortest-Path Games},
	booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial
	Intelligence, {IJCAI} 2021, Virtual Event / Montreal, Canada, 19-27
	August 2021},
	pages     = {4182--4189},
	publisher = {ijcai.org},
	year      = {2021},
	url       = {https://doi.org/10.24963/ijcai.2021/575},
	doi       = {10.24963/ijcai.2021/575},
	timestamp = {Wed, 25 Aug 2021 17:11:16 +0200},
	biburl    = {https://dblp.org/rec/conf/ijcai/TomasekHABC21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{chu2019multiagent,
	title={Multi-Agent Deep Reinforcement Learning for Large-scale Traffic Signal Control}, 
	author={Tianshu Chu and Jie Wang and Lara Codecà and Zhaojian Li},
	year={2019},
	eprint={1903.04527},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{SAC,
	title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
	author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
	year={2018},
	eprint={1801.01290},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{DQN,
	title={Dueling Network Architectures for Deep Reinforcement Learning}, 
	author={Ziyu Wang and Tom Schaul and Matteo Hessel and Hado van Hasselt and Marc Lanctot and Nando de Freitas},
	year={2016},
	eprint={1511.06581},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{deepmindblog,
	title={Traffic prediction with advanced Graph Neural Networks}, 
	author={Deepmind and Google},
	year={2020}
}
